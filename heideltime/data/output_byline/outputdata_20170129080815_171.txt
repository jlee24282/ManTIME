<?xml version="1.0"?>
<!DOCTYPE TimeML SYSTEM "TimeML.dtd">
<TimeML>
by Joseph Rickert Drew Linzer, the Bayesian statistician who attracted considerable attention <TIMEX3 tid="t2" type="DATE" value="2016">last year</TIMEX3> with his spot-on, R-based forecast of the <TIMEX3 tid="t1" type="DATE" value="2012">2012</TIMEX3> presidential election, <TIMEX3 tid="t3" type="DATE" value="PAST_REF">recently</TIMEX3> gave a tutorial on Bayesian statistics to the Bay Area useR Group (BARUG). Drew covered quite a bit of ground running R code that showed how to make use of WinBugs, JAGS and Stan, the major engines for specifying and solving Bayesian models. With this very helpful introduction to Bayesian thinking fresh in my mind I thought that it would be worth a look at how well R and computational Bayesian statistics are getting along. After all, they practically grew up together. The modern era of Bayesian computation began with the “rediscovery” of MCMC algorithms in <TIMEX3 tid="t6" type="DATE" value="199" mod="START">the early 1990s</TIMEX3>, and R became an open source project in <TIMEX3 tid="t4" type="DATE" value="1995">1995</TIMEX3>. The logical place to start, the CRAN Task View for Bayesian Inference, states that: Applied researchers interested in Bayesian statistics are increasingly attracted to R because of the ease of which one can code algorithms to sample from posterior distributions as well as the significant number of packages contributed to the Comprehensive R Archive Network (CRAN) that provide tools for Bayesian inference. My bet is that the reason that Bayesian task view lists 7 packages for general model fitting and over 80 packages for for specific Bayesian models and methods is because there are quite a few Bayesian statisticians are working in R with only a relatively small number are submitting packages to CRAN. However, even with all of this custom Bayesian coding it is probably safe to say that in the much of the computational heavy lifting for Bayesian analysis is accomplished through specialized Bayesian engines for which Drew provided examples.  WinBUGS is a version of the original BUGS (Bayesian Inference Using Gibbs Sampling) that began as a project  MRC Biostatistics Unit, Cambridge in <TIMEX3 tid="t7" type="DATE" value="1989">1989</TIMEX3>. WinBUGS is free, stable software that may be accessed via the R2WinBUGS R package. Although WinBUGS still sees quite a bit of use, all of the BUGS development these days is centered around the open source (GPL2) OpenBUGS software that runs on Unix and Linux in addition to Windows. The R2OpenBUGS package provides access to OpenBugs in a similar way that R2WinBUGS interfaces to WinBUGS. The BRugs package provides tighter integration to OpenBUGS. Its interactive interface allows for completing some housekeeping tasks such as checking convergence without having to exit the BUGS software. JAGS (Just Another Gibbs Sampler) is a Bayesian engine that was designed to work with R. It is open source (GPL 2) runs on multiple operating systems, is extensible and allows users to write their own functions. The language JAGS uses to specify Bayesian models is a variation of the basic BUGS language. R users access JAGS through the rjags package and may used the coda package to analyze the MCMC results. The newest Bayesian engine, Stan (named after Stanislaw Ulam of Monte Carlo fame), is a open source software (GPL 3) that was designe to be faster and handle hierarchical models that are out of reach for both OpenBugs and JAGS. Stan runs on various versions of Windows, Mac OS X and Linux.  According to the Stan manual: “A Stan program is first compiled to a C++ program by the Stan compiler stanc, then the C++ program compiled to a self-contained platform-specific executable.” Stan’s language for expressing the Bayesian models and its method computing posteriors (No-U-Turn sampler, a variant of Hamiltonian Monte Carlo) are also different from the BUGS. R users access Stan through the RStan package. If you are new to Bayesian Computing Drew’s sample code is a good place to start. Here are three lines of code to do a Bayesian regression that may open up an new way of thinking about regression inference for you. library(MCMCpack) breg <- mcmcregress(incumbent.vote ~ gdp.growth + net.approval two.terms, dat) summary(breg); plot(breg) created by pretty r at inside-r.org <TIMEX3 tid="t17" type="DATE" value="PRESENT_REF">now</TIMEX3> instead of a point estimate and p-value for coefficient you have an entire distribution to consider. a very nice expositional touch in drew’s tutorial is the use rstudio’s manipulate package create slider allowing one see how changing shape parameters beta prior affect posterior distribution. i think this intuition building tools helpful. drew mentioned couple books help go further: "the bugs book: practical introduction bayesian analysis" (<TIMEX3 tid="t18" type="DATE" value="2012">2012</TIMEX3>) david lunn et al. john kruschke's "doing data analysis: with bugs" (<TIMEX3 tid="t19" type="DATE" value="2010">2010</TIMEX3>). these would add: jim albert's classic "bayesian computation r" (<TIMEX3 tid="t20" type="DATE" value="2009">2009</TIMEX3>). two new r-based are "applied statistics openbugs" (<TIMEX3 tid="t21" type="DATE" value="2013">2013</TIMEX3>) mary kathryn cowles essentials jean-michel marin christian robert (<TIMEX3 tid="t22" type="DATE" value="2014">2014</TIMEX3>).  if there "killer app" drive someone analysis necessity my vote be hierarchical regression modeling. "data using multilevel> 

CIO, CTO & Developer Resources
</TimeML>
